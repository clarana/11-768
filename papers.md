Practical applications
----
- [Natural Language Processing with Small Feed-Forward Networks](https://arxiv.org/abs/1708.00214)
- [Machine Learning at Facebook: Understanding Inference at the Edge](https://research.fb.com/wp-content/uploads/2018/12/Machine-Learning-at-Facebook-Understanding-Inference-at-the-Edge.pdf)
- [Recognizing People in Photos Through Private On-Device Machine Learning](https://machinelearning.apple.com/research/recognizing-people-photos)
- [Knowledge Transfer for Efficient On-device False Trigger Mitigation](https://arxiv.org/abs/2010.10591)
- [Smart Reply: Automated Response Suggestion for Email](https://arxiv.org/abs/1606.04870)
- [Chat Smarter with Allo](https://ai.googleblog.com/2016/05/chat-smarter-with-allo.html)

Distillation
----
- [Model Compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)
- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
- [TinyBERT: Distilling BERT for Natural Language Understanding](https://aclanthology.org/2020.findings-emnlp.372/)
- [DistilBERT](https://arxiv.org/abs/1910.01108)
- [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://www.aclweb.org/anthology/2020.acl-main.195/)
- [Distilling Large Language Models into Tiny and Effective Students using pQRNN](https://arxiv.org/abs/2101.08890)
- [Sequence-Level Knowledge Distillation](https://arxiv.org/abs/1606.07947)
- [DynaBERT: Dynamic BERT with Adaptive Width and Depth](https://proceedings.neurips.cc/paper/2020/hash/6f5216f8d89b086c18298e043bfe48ed-Abstract.html)
- [Does Knowledge Distillation Really Work?](https://proceedings.neurips.cc/paper/2021/hash/376c6b9ff3bedbbea56751a84fffc10c-Abstract.html)

Pruning
----
- [Huge list of papers on neural network pruning](https://github.com/he-y/Awesome-Pruning)
- [Optimal Brain Damage](https://papers.nips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)
- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)
- [The Lottery Ticket Hypothesis: A Survey (blog post)](https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/)
- [Bayesian Bits: Unifying Quantization and Pruning](https://arxiv.org/abs/2005.07093)
- [Structured Pruning of Neural Networks with Budget-Aware Regularization](https://ieeexplore.ieee.org/abstract/document/8953545)
- [Block Pruning For Faster Transformers](https://aclanthology.org/2021.emnlp-main.829/)
- [Structured Pruning Learns Compact and Accurate Models](https://aclanthology.org/2022.acl-long.107/)
- [Drawing Early-Bird Tickets: Towards More Efficient Training of Deep Networks](https://openreview.net/forum?id=BJxsrgStvr)
- [Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models](https://openreview.net/forum?id=Nfl-iXa-y7R)

Neural architecture search
----
- [SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers](https://arxiv.org/abs/1905.12107)
- [FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable NAS](https://arxiv.org/abs/1812.03443)
- [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)
- [High-Performance Large-Scale Image Recognition Without Normalization](https://arxiv.org/abs/2102.06171)
- [HAT: Hardware-Aware Transformers for Efficient Natural Language Processing](https://arxiv.org/abs/2005.14187)

Benchmarking
----
- [Show Your Work: Improved Reporting of Experimental Results](https://aclanthology.org/D19-1224/)
- [Showing Your Work Doesn’t Always Work](https://aclanthology.org/2020.acl-main.246/)
- [The Hardware Lottery](https://arxiv.org/abs/2009.06489)
- [HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing](https://arxiv.org/abs/2002.05829)
- [An Analysis of Deep Neural Network Models for Practical Applications](https://arxiv.org/abs/1605.07678)
- [MLPerf Inference Benchmark](https://arxiv.org/abs/1911.02549)
- [MLPerf Training Benchmark](https://arxiv.org/abs/1910.01500)
- [Roofline: an insightful visual performance model for multicore architectures](https://people.eecs.berkeley.edu/~kubitron/cs252/handouts/papers/RooflineVyNoYellow.pdf)
- [Evaluating the Energy Efficiency of Deep Convolutional Neural Networks on CPUs and GPUs](https://ieeexplore.ieee.org/document/7723730)
- [Deep Learning Language Modeling Workloads: Where Time Goes on Graphics Processors](https://ieeexplore.ieee.org/document/9041972)
- [IrEne: Interpretable Energy Prediction for Transformers](https://aclanthology.org/2021.acl-long.167/)
- [Expected Validation Performance and Estimation of a Random Variable's Maximum](https://aclanthology.org/2021.findings-emnlp.342/)
- [The Efficiency Misnomer](https://openreview.net/forum?id=iulEMLYh1uR)
- [NeuralPower: Predict and Deploy Energy-Efficient Convolutional Neural Networks](https://proceedings.mlr.press/v77/cai17a.html)
- [Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression](https://aclanthology.org/2021.emnlp-main.832/)

Quantization
----
- [Scalable Methods for 8-bit Training of Neural Networks](https://arxiv.org/abs/1805.11046)
- [Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](https://arxiv.org/abs/2002.11794)
- [Once-for-All: Train One Network and Specialize it for Efficient Deployment](https://arxiv.org/abs/1908.09791)
- [Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT](https://arxiv.org/abs/1909.05840)
- [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321)
- [BinaryBERT](https://aclanthology.org/2021.acl-long.334/)
- [TernaryBERT: Distillation-aware Ultra-low Bit BERT](https://www.aclweb.org/anthology/2020.emnlp-main.37/)
- [Binarized Neural Networks](https://arxiv.org/abs/1602.02830)
- [Training Deep Neural Networks with 8-bit Floating Point Numbers](https://arxiv.org/abs/1812.08011)
- [HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision](https://arxiv.org/abs/1905.03696)
- [Profile-Driven Automated Mixed Precision](https://arxiv.org/abs/1606.00251)
- [Mixed Precision Training](https://openreview.net/forum?id=r1gs9JgRZ)
- [Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation](https://arxiv.org/abs/1308.3432)
- [Differentiable Model Compression via Pseudo Quantization Noise](https://arxiv.org/abs/2104.09987)
- [Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets](https://openreview.net/forum?id=Skh4jRcKQ)

Multimodal
----
- [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/abs/1908.03557)
- [Mapping Navigation Instructions to Cont. Control Actions with Position-Visitation Pred.](https://arxiv.org/abs/1811.04179)
- [Early Fusion for Goal Directed Robotic Vision](https://arxiv.org/abs/1811.08824)

Architecture-specific tricks: CNNs
----
- [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/abs/1603.05279)
- [XOR-Net: An Efficient Computation Pipeline for Binary Neural Network Inference on Edge Devices](https://ieeexplore.ieee.org/document/9359148)
- [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)
- [Fast Convolutional Nets With fbfft: A GPU Performance Evaluation](https://arxiv.org/abs/1412.7580)
- [FFT Convolutions are Faster than Winograd on Modern CPUs, Here’s Why](https://arxiv.org/abs/1809.07851)
- [Fast Algorithms for Convolutional Neural Networks](https://arxiv.org/abs/1509.09308)

Architecture-specific tricks: Softmax
----
- [Efficient softmax approximation for GPUs](https://arxiv.org/abs/1609.04309)

Architecture-specific tricks: Embeddings/inputs
----
- [Adaptive Input Representations for Neural Language Modeling](https://arxiv.org/abs/1809.1085)
- [Embedding Recycling for Language Models](https://arxiv.org/abs/2207.04993)

Task-specific tricks
----
- [A Study of Non-autoregressive Model for Sequence Generation](https://arxiv.org/abs/2004.10454)
- [Mask-Predict: Parallel Decoding of Conditional Masked Language Models](https://arxiv.org/abs/1904.09324)
- [Non-autoregressive neural machine translation](https://arxiv.org/abs/1711.02281)
- [Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation](https://arxiv.org/abs/2006.10369)
- [Improving Low Compute Language Modeling with In-Domain Embedding Initialisation](https://arxiv.org/abs/2009.14109)
- [COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List](https://aclanthology.org/2021.naacl-main.241/)

Architecture-specific tricks: Transformers
----
- [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236)
- [Do Transformer Modifications Transfer Across Implementations and Applications?](https://arxiv.org/abs/2102.11972)
- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)
- [Consistent Accelerated Inference via Confident Adaptive Transformers](https://arxiv.org/abs/2104.08803)
- [PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination](https://arxiv.org/abs/2001.08950)
- [Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level](https://arxiv.org/abs/2105.06020)
- [Are Sixteen Heads Really Better Than One?](http://papers.nips.cc/paper/9551-are-sixteen-heads-really-better-than-one)
- [Are Pre-trained Convolutions Better than Pre-trained Transformers?](https://aclanthology.org/2021.acl-long.335/)
- [FNet: Mixing Tokens with Fourier Transforms](https://aclanthology.org/2022.naacl-main.319/)

Speech
----
- [Speech recognition with deep recurrent neural networks](https://ieeexplore.ieee.org/abstract/document/6638947)
- [Streaming End-to-end Speech Recognition For Mobile Devices](https://arxiv.org/abs/1811.06621)

Accelerating training
----
- [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)
- [Pre-Training Transformers as Energy-Based Cloze Models](https://arxiv.org/abs/2012.08561)
- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)
- [Accelerating Deep Learning by Focusing on the Biggest Losers](https://arxiv.org/abs/1910.00762)
- [Dataset Distillation](https://arxiv.org/abs/1811.10959)
- [Competence-based curriculum learning for neural machine translation](https://arxiv.org/abs/1903.09848)
- [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)
- [AutoFL: Enabling Heterogeneity-Aware Energy Efficient Federated Learning](https://arxiv.org/abs/2107.08147)

Carbon footprint and alternative power sources
----
- [Tackling Climate Change with Machine Learning](https://arxiv.org/abs/1906.05433)
- [Aligning artificial intelligence with climate change mitigation](https://www.nature.com/articles/s41558-022-01377-7)
- [Energy and Policy Considerations for Deep Learning in NLP](https://aclanthology.org/P19-1355/)
- [On the opportunities and risks of foundation models (Section 5.3)](https://arxiv.org/abs/2108.07258)
- [Quantifying the Carbon Emissions of Machine Learning](https://arxiv.org/abs/1910.09700)
- [Measuring the Carbon Intensity of AI in Cloud Instances](https://dl.acm.org/doi/10.1145/3531146.3533234)
- [Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning](https://jmlr.org/papers/v21/20-312.html)
- [Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models](https://arxiv.org/abs/2007.03051)

